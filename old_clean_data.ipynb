{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter, defaultdict\n",
    "import csv\n",
    "import pickle\n",
    "import re\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = 'train.csv'\n",
    "train_fieldnames = ['pid', 'survived', 'pclass', 'name', 'sex', 'age', 'sisp', 'pach', 'ticket', 'fare', 'cabin', 'port']\n",
    "test_path = 'test.csv'\n",
    "test_fieldnames = ['pid', 'pclass', 'name', 'sex', 'age', 'sisp', 'pach', 'ticket', 'fare', 'cabin', 'port']\n",
    "clean_train_path = 'cleaned_train.csv'\n",
    "clean_train_fieldnames = ['pid', 'survived', 'pclass', 'honor', 'origin', 'sex', 'age', 'sisp', 'pach', 'ticket', 'fare', 'cabin', 'port']\n",
    "clean_test_path = 'cleaned_test.csv'\n",
    "clean_test_fieldnames = ['pid', 'pclass', 'honor', 'origin', 'sex', 'age', 'sisp', 'pach', 'ticket', 'fare', 'cabin', 'port']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all values in input data as dict {fieldname: set_of_values}\n",
    "def get_values(train_path, train_fieldnames, test_path, test_fieldnames):\n",
    "    input_values = defaultdict(set)\n",
    "    csv.register_dialect('mixed', delimiter=',', escapechar=None, quoting=csv.QUOTE_MINIMAL)\n",
    "    with open(train_path, mode='rt', errors='ignore') as train, open(test_path, mode='rt', errors='ignore') as test:\n",
    "        train_reader = csv.DictReader(train, fieldnames=train_fieldnames, dialect='mixed')\n",
    "        test_reader = csv.DictReader(test, fieldnames=test_fieldnames, dialect='mixed')\n",
    "        next(train_reader, None) # skip header\n",
    "        next(test_reader, None) # skip header\n",
    "        for row in train_reader:\n",
    "            for k, v in row.items():\n",
    "                input_values[k].add(v)\n",
    "        for row in test_reader:\n",
    "            for k, v in row.items():\n",
    "                input_values[k].add(v)\n",
    "    return input_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_values = get_values(train_path, train_fieldnames, test_path, test_fieldnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['pid', 'survived', 'pclass', 'name', 'sex', 'age', 'sisp', 'pach', 'ticket', 'fare', 'cabin', 'port'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_values.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion functions for cleaning data\n",
    "identity = lambda x: x\n",
    "\n",
    "def ints(values):\n",
    "    values.discard('')\n",
    "    conversion = {k: i for i, k in enumerate(values)}\n",
    "    conversion[''] = len(conversion)\n",
    "    return lambda v: conversion[v]\n",
    "\n",
    "def bins(uppers):\n",
    "    def conversion(v):\n",
    "        last = len(uppers)\n",
    "        if v == '':\n",
    "            return last + 1\n",
    "        v = int(float(v))\n",
    "        for i, upper in enumerate(uppers):\n",
    "            if v < upper:\n",
    "                return i\n",
    "        return last\n",
    "    return conversion\n",
    "\n",
    "honorifics = {\n",
    "    'Mr' : 1,\n",
    "    'Don': 2, 'Sir': 2, 'Rev': 2, 'Dr': 2, 'Major': 2, 'Col': 2, 'Capt': 2, 'Jonkheer': 2,\n",
    "    'Master': 3,\n",
    "    'Mlle': 4, 'Ms': 4, 'Miss': 4,\n",
    "    'Mrs': 5, 'Mme': 5, 'Dona': 5, 'Lady': 5, 'the Countess': 5\n",
    "}\n",
    "\n",
    "with open('name_origins.pickle', 'rb') as file:\n",
    "    first_name_origins, last_name_origins = pickle.load(file)\n",
    "\n",
    "origin_map = {\n",
    "    'unknown': 5,\n",
    "    'English': 0, 'Irish': 0, 'Cornish': 0,  'Welsh': 0, 'Scottish': 0,\n",
    "    'French': 1, 'Romance': 1, 'Italian': 1, 'Portuguese': 1, 'Spanish': 1, 'Catalan': 1, 'Roman': 1,\n",
    "    'Swedish': 2, 'Finnish': 2, 'Danish': 2, 'Norwegian': 2, 'Icelandic': 2, 'Mythology': 2,\n",
    "    'German': 3, 'Dutch': 3, 'Polish': 3,  'Hungarian': 3, 'Czech': 3, 'Slovak': 3,\n",
    "    'Hebrew': 4, 'Biblical': 4, 'Yiddish': 4, 'Jewish': 4,\n",
    "    'Armenian': 4, 'Georgian': 4, 'Slovene': 4, 'Turkish': 4,\n",
    "    'Russian': 4, 'Greek': 4, 'Serbian': 4, 'Bulgarian': 4,\n",
    "    'Ukrainian': 4, 'Bosnian': 4, 'Lithuanian': 4, 'Croatian': 4, 'Estonian': 4,\n",
    "    'Punjabi': 4, 'Culture': 4, 'Chinese': 4, 'Urdu': 4, 'African': 4, 'Arabic': 4,\n",
    "}\n",
    "\n",
    "def split_name(name):\n",
    "    match = re.search(r'(\\w+), ([\\w+ ]+)\\. \\(?(\\w+)', name)\n",
    "    if not match:\n",
    "        print('Regex Error: ', name)\n",
    "        return None\n",
    "    honor = honorifics[match.group(2)]\n",
    "    lorigin = last_name_origins[match.group(1)]\n",
    "    forigin = first_name_origins[match.group(3)]\n",
    "    last = origin_map[lorigin]\n",
    "    first = origin_map[forigin]\n",
    "    if first == last:\n",
    "        origin = first\n",
    "    elif first == 'unknown':\n",
    "        origin = last\n",
    "    elif last == 'unknown':\n",
    "        origin = first\n",
    "    else:\n",
    "        origin = first\n",
    "    return (honor, origin)\n",
    "\n",
    "def split_ticket(ticket):\n",
    "    match = re.search(r'\\d+$|LINE', ticket)\n",
    "    if not match:\n",
    "        print('Ticket Error: ', ticket)\n",
    "        return None\n",
    "    length = len(match.group(0))\n",
    "    return 4 if length <= 4 else length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "converters = {\n",
    "    'pid': identity,\n",
    "    'survived': identity,\n",
    "    'pclass': ints(input_values['pclass']),\n",
    "    'name': split_name,\n",
    "    'sex': ints(input_values['sex']),\n",
    "    'age': bins([3, 6, 9, 12, 15, 18, 21, 24, 27, 30, 35, 40, 50]),\n",
    "    'sisp': bins([1, 2, 3]),\n",
    "    'pach': bins([1]),\n",
    "    'ticket': split_ticket,\n",
    "    'fare': bins([10, 20, 30, 50, 80]),\n",
    "    'cabin': lambda s: 1 if s else 0,\n",
    "    'port': ints(input_values['port'])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean data and output to file for further processing\n",
    "def clean_data(in_path, in_fieldnames, out_path, out_fieldnames):\n",
    "    csv.register_dialect('ints', delimiter=',', escapechar=None, quoting=csv.QUOTE_NONE)\n",
    "    csv.register_dialect('ints', delimiter=',', escapechar=None, quoting=csv.QUOTE_NONE)\n",
    "    with open(in_path, mode='rt', errors='ignore') as in_file, open(out_path, mode='a+', errors='ignore') as out_file:\n",
    "        reader = csv.DictReader(in_file, fieldnames=in_fieldnames, dialect='mixed')\n",
    "        next(reader, None) # skip header\n",
    "        writer = csv.DictWriter(out_file, fieldnames=out_fieldnames, dialect='ints')\n",
    "        out_file.truncate(0) # delete any file contents\n",
    "        writer.writeheader()\n",
    "        for row in reader:\n",
    "            iter_items = list(row.items())\n",
    "            for k, v in iter_items:\n",
    "                if k == 'name':\n",
    "                    row['honor'], row['origin'] = converters[k](v)\n",
    "                else:\n",
    "                    row[k] = converters[k](v)\n",
    "            del row['name']\n",
    "            writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data(train_path, train_fieldnames, clean_train_path, clean_train_fieldnames)\n",
    "clean_data(test_path, test_fieldnames, clean_test_path, clean_test_fieldnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle results and don't repeat unless necessary!\n",
    "# Make conversion maps for name origins\n",
    "def make_name_maps():\n",
    "    names = list(input_values['name'])\n",
    "    fnames = set()\n",
    "    lnames = set()\n",
    "    for name in names:\n",
    "        match = re.search(r'(\\w+),[\\w+ ]+\\. \\(?(\\w+)', name)\n",
    "        if match:\n",
    "            lnames.add(match.group(1))\n",
    "            fnames.add(match.group(2))\n",
    "        else:\n",
    "            print('Regex Error: ', name)\n",
    "    fnames = list(fnames)\n",
    "    lnames = list(lnames)\n",
    "    forigins = defaultdict(dict)\n",
    "    lorigins = defaultdict(dict)\n",
    "    with requests.session() as s:\n",
    "        for name in fnames:\n",
    "            forigins[name] = name_origin(name, s, surname=0)\n",
    "        for name in lnames:\n",
    "            lorigins[name] = name_origin(name, s, surname=1)\n",
    "    return (forigins, lorigins)\n",
    "\n",
    "def name_origin(name, session, surname=0):\n",
    "    url = ('https://www.behindthename.com/name/', 'https://surnames.behindthename.com/name/')[surname]\n",
    "    response = session.get(url + name.strip(), stream=True)\n",
    "    if response.status_code != 200:\n",
    "        return 'unknown'\n",
    "    raw_html = response.content\n",
    "    html = BeautifulSoup(raw_html, 'html.parser')\n",
    "    usage = html.find('a', attrs={'class': 'usg'})\n",
    "    if not usage:\n",
    "        return 'unknown'\n",
    "    usage = re.sub(r' \\(.*\\)', '', usage.string)\n",
    "    split = usage.split()\n",
    "    if len(split) > 1:\n",
    "        usage = split[-1]\n",
    "    return usage\n",
    "\n",
    "#forig, lorig = make_name_maps()\n",
    "#to_save = [forig, lorig]\n",
    "#with open('name_origins.pickle', 'wb') as dump:\n",
    "#    pickle.dump(to_save, dump)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
